{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffb1cc6-3550-4be2-8a88-e9dd9932c3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run VECTOR_DB_BUILD.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320f4157-76a3-49b6-8c44-95d7c98e8583",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoModelForCausalLM, BitsAndBytesConfig, AutoTokenizer\n",
    "import accelerate\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fc03ef-c54e-4d3f-b12c-8342d71a5a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -i https://pypi.org/simple/ bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368697bd-fd07-4f34-b897-b499b21677b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "READER_MODEL_NAME = \"HuggingFaceH4/zephyr-7b-beta\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    READER_MODEL_NAME, quantization_config=bnb_config\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(READER_MODEL_NAME)\n",
    "\n",
    "READER_LLM = pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    task=\"text-generation\",\n",
    "    do_sample=True,\n",
    "    temperature=0.2,\n",
    "    repetition_penalty=1.1,\n",
    "    return_full_text=False,\n",
    "    max_new_tokens=500,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cc942b-a909-4141-a717-9a9914643e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_in_chat_format = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"\"\"Using the information contained in the context,\n",
    "give a comprehensive answer to the question.\n",
    "Respond only to the question asked, response should be concise and relevant to the question.\n",
    "Provide the number of the source document when relevant.\n",
    "If the answer cannot be deduced from the context, do not give an answer.\"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"\"\"Context:\n",
    "{context}\n",
    "---\n",
    "Now here is the question you need to answer.\n",
    "\n",
    "Question: {question}\"\"\",\n",
    "    },\n",
    "]\n",
    "RAG_PROMPT_TEMPLATE = tokenizer.apply_chat_template(\n",
    "    prompt_in_chat_format, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "print(RAG_PROMPT_TEMPLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab957bac-422e-40ca-afcb-ffc2554e6636",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_generator(questions, history):\n",
    "    retrieved_docs = main_retriever.get_relevant_documents(questions)\n",
    "    retrieved_docs_text = [\n",
    "        doc.page_content for doc in retrieved_docs\n",
    "    ]  # we only need the text of the documents\n",
    "    context = \"\\nExtracted documents:\\n\"\n",
    "    context += \"\".join(\n",
    "        [f\"Document {str(i)}:::\\n\" + doc for i, doc in enumerate(retrieved_docs_text)]\n",
    "    )\n",
    "    final_prompt = RAG_PROMPT_TEMPLATE.format(\n",
    "        question=questions, context=context\n",
    "    )\n",
    "        \n",
    "    # Redact an answer\n",
    "    answer = READER_LLM(final_prompt)[0][\"generated_text\"]\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fdf0fd-c524-4948-b3b4-6d3899cb4036",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
