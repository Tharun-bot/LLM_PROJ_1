{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ffb1cc6-3550-4be2-8a88-e9dd9932c3d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/baskar/miniconda3/envs/llm/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/baskar/miniconda3/envs/llm/lib/python3.12/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "%run VECTOR_DB_BUILD.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "320f4157-76a3-49b6-8c44-95d7c98e8583",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoModelForCausalLM, BitsAndBytesConfig, AutoTokenizer\n",
    "import accelerate\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6fc03ef-c54e-4d3f-b12c-8342d71a5a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple/\n",
      "Requirement already satisfied: bitsandbytes in /home/baskar/miniconda3/envs/llm/lib/python3.12/site-packages (0.43.1)\n",
      "Requirement already satisfied: torch in /home/baskar/miniconda3/envs/llm/lib/python3.12/site-packages (from bitsandbytes) (2.3.0)\n",
      "Requirement already satisfied: numpy in /home/baskar/miniconda3/envs/llm/lib/python3.12/site-packages (from bitsandbytes) (1.26.4)\n",
      "Requirement already satisfied: filelock in /home/baskar/miniconda3/envs/llm/lib/python3.12/site-packages (from torch->bitsandbytes) (3.13.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/baskar/miniconda3/envs/llm/lib/python3.12/site-packages (from torch->bitsandbytes) (4.9.0)\n",
      "Requirement already satisfied: sympy in /home/baskar/miniconda3/envs/llm/lib/python3.12/site-packages (from torch->bitsandbytes) (1.12)\n",
      "Requirement already satisfied: networkx in /home/baskar/miniconda3/envs/llm/lib/python3.12/site-packages (from torch->bitsandbytes) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/baskar/miniconda3/envs/llm/lib/python3.12/site-packages (from torch->bitsandbytes) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /home/baskar/miniconda3/envs/llm/lib/python3.12/site-packages (from torch->bitsandbytes) (2024.3.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/baskar/miniconda3/envs/llm/lib/python3.12/site-packages (from torch->bitsandbytes) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/baskar/miniconda3/envs/llm/lib/python3.12/site-packages (from torch->bitsandbytes) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/baskar/miniconda3/envs/llm/lib/python3.12/site-packages (from torch->bitsandbytes) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/baskar/miniconda3/envs/llm/lib/python3.12/site-packages (from torch->bitsandbytes) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/baskar/miniconda3/envs/llm/lib/python3.12/site-packages (from torch->bitsandbytes) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/baskar/miniconda3/envs/llm/lib/python3.12/site-packages (from torch->bitsandbytes) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/baskar/miniconda3/envs/llm/lib/python3.12/site-packages (from torch->bitsandbytes) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/baskar/miniconda3/envs/llm/lib/python3.12/site-packages (from torch->bitsandbytes) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/baskar/miniconda3/envs/llm/lib/python3.12/site-packages (from torch->bitsandbytes) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/baskar/miniconda3/envs/llm/lib/python3.12/site-packages (from torch->bitsandbytes) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/baskar/miniconda3/envs/llm/lib/python3.12/site-packages (from torch->bitsandbytes) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/baskar/miniconda3/envs/llm/lib/python3.12/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/baskar/miniconda3/envs/llm/lib/python3.12/site-packages (from jinja2->torch->bitsandbytes) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/baskar/miniconda3/envs/llm/lib/python3.12/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -i https://pypi.org/simple/ bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662cb15a-cae2-4236-9780-9255657fb15b",
   "metadata": {},
   "source": [
    "# Reader - LLM ðŸ’¬\n",
    "In this part, the LLM Reader reads the retrieved context to formulate its answer.\n",
    "\n",
    "There are actually substeps that can all be tuned:\n",
    "\n",
    "The content of the retrieved documents is aggregated together into the \"context\", with many processing options like prompt compression.\n",
    "The context and the user query are aggregated into a prompt then given to the LLM to generate its answer.\n",
    "2.1. Reader model\n",
    "The choice of a reader model is important on a few aspects:\n",
    "\n",
    "the reader model's max_seq_length must accomodate our prompt, which includes the context output by the retriever call: the context consists in 5 documents of 512 tokens each, so we aim for a context length of 4k tokens at least.\n",
    "the reader model\n",
    "For this example, we chose HuggingFaceH4/zephyr-7b-beta, a small but powerful model.\n",
    "\n",
    "With many models being released every week, you may want to substitute this model to the latest and greatest. The best way to keep track of open source LLMs is to check the Open-source LLM leaderboard.\n",
    "\n",
    "To make inference faster, we will load the quantized version of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "368697bd-fd07-4f34-b897-b499b21677b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:06<00:00,  1.27it/s]\n"
     ]
    }
   ],
   "source": [
    "READER_MODEL_NAME = \"HuggingFaceH4/zephyr-7b-beta\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    READER_MODEL_NAME, quantization_config=bnb_config\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(READER_MODEL_NAME)\n",
    "\n",
    "READER_LLM = pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    task=\"text-generation\",\n",
    "    do_sample=True,\n",
    "    temperature=0.2,\n",
    "    repetition_penalty=1.1,\n",
    "    return_full_text=False,\n",
    "    max_new_tokens=500,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e256a198-e70f-48a3-bac1-c95779b45e79",
   "metadata": {},
   "source": [
    "# Prompt\n",
    "The RAG prompt template below is what we will feed to the Reader LLM: it is important to have it formatted in the Reader LLM's chat template.\n",
    "\n",
    "We give it our context and the user's question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8cc942b-a909-4141-a717-9a9914643e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "Using the information contained in the context,\n",
      "give a comprehensive answer to the question.\n",
      "Respond only to the question asked, response should be concise and relevant to the question.\n",
      "Provide the number of the source document when relevant.\n",
      "If the answer cannot be deduced from the context, do not give an answer.</s>\n",
      "<|user|>\n",
      "Context:\n",
      "{context}\n",
      "---\n",
      "Now here is the question you need to answer.\n",
      "\n",
      "Question: {question}</s>\n",
      "<|assistant|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt_in_chat_format = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"\"\"Using the information contained in the context,\n",
    "give a comprehensive answer to the question.\n",
    "Respond only to the question asked, response should be concise and relevant to the question.\n",
    "Provide the number of the source document when relevant.\n",
    "If the answer cannot be deduced from the context, do not give an answer.\"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"\"\"Context:\n",
    "{context}\n",
    "---\n",
    "Now here is the question you need to answer.\n",
    "\n",
    "Question: {question}\"\"\",\n",
    "    },\n",
    "]\n",
    "RAG_PROMPT_TEMPLATE = tokenizer.apply_chat_template(\n",
    "    prompt_in_chat_format, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "print(RAG_PROMPT_TEMPLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab957bac-422e-40ca-afcb-ffc2554e6636",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_generator(questions, history):\n",
    "    retrieved_docs = main_retriever.get_relevant_documents(questions)\n",
    "    retrieved_docs_text = [\n",
    "        doc.page_content for doc in retrieved_docs\n",
    "    ]  # we only need the text of the documents\n",
    "    context = \"\\nExtracted documents:\\n\"\n",
    "    context += \"\".join(\n",
    "        [f\"Document {str(i)}:::\\n\" + doc for i, doc in enumerate(retrieved_docs_text)]\n",
    "    )\n",
    "    final_prompt = RAG_PROMPT_TEMPLATE.format(\n",
    "        question=questions, context=context\n",
    "    )\n",
    "        \n",
    "    # Redact an answer\n",
    "    answer = READER_LLM(final_prompt)[0][\"generated_text\"]\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fdf0fd-c524-4948-b3b4-6d3899cb4036",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
